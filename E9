import numpy as np

class LinearRegression:
    """Simple Linear Regression implementation"""
    def __init__(self, learning_rate=0.01, iterations=1000):
        self.lr = learning_rate
        self.iterations = iterations
        self.weights = None
        self.bias = None
        self.loss_history = []
    
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        for i in range(self.iterations):
            y_pred = np.dot(X, self.weights) + self.bias
            
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            self.weights -= self.lr * dw
            self.bias -= self.lr * db
            
            loss = np.mean((y_pred - y) ** 2)
            self.loss_history.append(loss)
    
    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

class PolynomialRegression:
    """Polynomial Regression implementation"""
    def __init__(self, degree=2, learning_rate=0.01, iterations=1000):
        self.degree = degree
        self.lr = learning_rate
        self.iterations = iterations
        self.weights = None
        self.bias = None
        self.loss_history = []
        self.poly_features = None
    
    def create_polynomial_features(self, X):
        """Create polynomial features up to specified degree"""
        n_samples = X.shape[0]
        X_poly = np.ones((n_samples, self.degree))
        
        for d in range(1, self.degree + 1):
            X_poly[:, d-1] = X.flatten() ** d
        
        return X_poly
    
    def fit(self, X, y):
        # Transform features to polynomial
        X_poly = self.create_polynomial_features(X)
        self.poly_features = X_poly.shape[1]
        
        n_samples = X_poly.shape[0]
        self.weights = np.zeros(self.poly_features)
        self.bias = 0
        
        for i in range(self.iterations):
            y_pred = np.dot(X_poly, self.weights) + self.bias
            
            dw = (1/n_samples) * np.dot(X_poly.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            self.weights -= self.lr * dw
            self.bias -= self.lr * db
            
            loss = np.mean((y_pred - y) ** 2)
            self.loss_history.append(loss)
    
    def predict(self, X):
        X_poly = self.create_polynomial_features(X)
        return np.dot(X_poly, self.weights) + self.bias
    
    def get_equation(self):
        """Get polynomial equation string"""
        equation = f"y = {self.bias:.4f}"
        for i in range(self.degree):
            coef = self.weights[i]
            if i == 0:
                equation += f" + {coef:.4f}x"
            else:
                equation += f" + {coef:.4f}x^{i+1}"
        return equation

# Utility functions
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

def r2_score(y_true, y_pred):
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    return 1 - (ss_res / ss_tot)

def train_test_split(X, y, test_size=0.2, random_state=None):
    if random_state:
        np.random.seed(random_state)
    
    n_samples = len(X)
    test_samples = int(n_samples * test_size)
    indices = np.arange(n_samples)
    np.random.shuffle(indices)
    
    test_idx = indices[:test_samples]
    train_idx = indices[test_samples:]
    
    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]

def generate_nonlinear_data(n_samples=100, noise=0.5, random_state=42):
    """Generate nonlinear data for demonstration"""
    np.random.seed(random_state)
    X = 6 * np.random.rand(n_samples, 1) - 3
    y = 0.5 * X**2 + X + 2 + noise * np.random.randn(n_samples, 1)
    return X, y.flatten()

def print_comparison_table(metrics_lr, metrics_pr, feature_counts):
    """Print formatted comparison table"""
    print("\n" + "=" * 70)
    print("COMPARISON: LINEAR vs POLYNOMIAL REGRESSION")
    print("=" * 70)
    print("\nCRITERION                 LINEAR REGRESSION    POLYNOMIAL REGRESSION")
    print("-" * 70)
    
    print(f"{'Model Complexity':<25} {'Simple':<20} {'More Complex'}")
    print(f"{'Features Used':<25} {feature_counts['linear']:<20} {feature_counts['poly']}")
    print(f"{'Equation Form':<25} {'y = mx + c':<20} {'y = a + bx + cx² + ...'}")
    print(f"{'Mean Squared Error':<25} {metrics_lr['mse']:<20.4f} {metrics_pr['mse']:.4f}")
    print(f"{'R² Score':<25} {metrics_lr['r2']:<20.4f} {metrics_pr['r2']:.4f}")
    print(f"{'Final Training Loss':<25} {metrics_lr['final_loss']:<20.4f} {metrics_pr['final_loss']:.4f}")
    print(f"{'Overfitting Risk':<25} {'Low':<20} {'High (with high degree)'}")
    print(f"{'Interpretability':<25} {'High':<20} {'Medium to Low'}")
    print(f"{'Best For':<25} {'Linear relationships':<20} {'Nonlinear relationships'}")
    
    print("\n" + "=" * 70)
    print("KEY INSIGHTS:")
    print("=" * 70)

def main():
    print("=" * 70)
    print("LINEAR vs POLYNOMIAL REGRESSION COMPARISON")
    print("=" * 70)
    
    # Generate nonlinear data
    print("\n1. GENERATING NON-LINEAR DATA")
    print("   Data follows: y = 0.5x² + x + 2 + noise")
    X, y = generate_nonlinear_data(n_samples=200, noise=0.8)
    print(f"   Generated {len(X)} samples")
    
    # Split data
    print("\n2. SPLITTING DATA")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    print(f"   Training samples: {len(X_train)}")
    print(f"   Test samples: {len(X_test)}")
    
    # Train Linear Regression
    print("\n3. TRAINING LINEAR REGRESSION")
    print("   " + "-" * 40)
    lr_model = LinearRegression(learning_rate=0.05, iterations=1000)
    lr_model.fit(X_train, y_train)
    lr_predictions = lr_model.predict(X_test)
    
    lr_metrics = {
        'mse': mean_squared_error(y_test, lr_predictions),
        'r2': r2_score(y_test, lr_predictions),
        'final_loss': lr_model.loss_history[-1] if lr_model.loss_history else 0,
        'equation': f"y = {lr_model.weights[0]:.4f}x + {lr_model.bias:.4f}"
    }
    
    print(f"   Equation: {lr_metrics['equation']}")
    print(f"   Test MSE: {lr_metrics['mse']:.4f}")
    print(f"   Test R²: {lr_metrics['r2']:.4f}")
    
    # Train Polynomial Regression (degree 2)
    print("\n4. TRAINING POLYNOMIAL REGRESSION (Degree 2)")
    print("   " + "-" * 40)
    pr_model = PolynomialRegression(degree=2, learning_rate=0.01, iterations=2000)
    pr_model.fit(X_train, y_train)
    pr_predictions = pr_model.predict(X_test)
    
    pr_metrics = {
        'mse': mean_squared_error(y_test, pr_predictions),
        'r2': r2_score(y_test, pr_predictions),
        'final_loss': pr_model.loss_history[-1] if pr_model.loss_history else 0,
        'equation': pr_model.get_equation()
    }
    
    print(f"   Equation: {pr_metrics['equation']}")
    print(f"   Test MSE: {pr_metrics['mse']:.4f}")
    print(f"   Test R²: {pr_metrics['r2']:.4f}")
    
    # Feature comparison
    feature_counts = {
        'linear': f"{X_train.shape[1]} feature(s)",
        'poly': f"{pr_model.poly_features} polynomial features"
    }
    
    # Print comparison table
    print_comparison_table(lr_metrics, pr_metrics, feature_counts)
    
    # Detailed analysis
    print("\nA. WHEN TO USE LINEAR REGRESSION:")
    print("   • Relationships are approximately linear")
    print("   • Simplicity and interpretability are important")
    print("   • Dataset is small or high-dimensional")
    print("   • Need fast predictions")
    print(f"   • Current R²: {lr_metrics['r2']:.4f} (lower than polynomial)")
    
    print("\nB. WHEN TO USE POLYNOMIAL REGRESSION:")
    print("   • Relationships are curved/nonlinear")
    print("   • Accuracy is more important than interpretability")
    print("   • Have sufficient data to prevent overfitting")
    print("   • Can use regularization (not implemented here)")
    print(f"   • Current R²: {pr_metrics['r2']:.4f} (higher than linear)")
    
    print("\nC. PERFORMANCE DIFFERENCE:")
    mse_diff = lr_metrics['mse'] - pr_metrics['mse']
    r2_diff = pr_metrics['r2'] - lr_metrics['r2']
    
    if mse_diff > 0:
        print(f"   • Polynomial is {mse_diff:.4f} MSE units better")
    else:
        print(f"   • Linear is {-mse_diff:.4f} MSE units better")
    
    print(f"   • R² improvement: {r2_diff:.4f}")
    
    print("\nD. OVERFITTING WARNING:")
    print("   • Polynomial regression with high degrees can overfit")
    print("   • Always validate with test data or cross-validation")
    print("   • Consider regularization techniques")
    
    # Demonstrate overfitting risk
    print("\n5. DEMONSTRATING OVERFITTING RISK")
    print("   " + "-" * 40)
    
    # Try very high degree polynomial
    high_degree_model = PolynomialRegression(degree=10, learning_rate=0.001, iterations=3000)
    high_degree_model.fit(X_train, y_train)
    high_degree_pred = high_degree_model.predict(X_test)
    
    high_degree_mse = mean_squared_error(y_test, high_degree_pred)
    high_degree_r2 = r2_score(y_test, high_degree_pred)
    
    print(f"   Degree 10 Polynomial:")
    print(f"   • Test MSE: {high_degree_mse:.4f}")
    print(f"   • Test R²: {high_degree_r2:.4f}")
    
    if high_degree_mse > pr_metrics['mse']:
        print("   ⚠️  Degree 10 performs WORSE than degree 2 (overfitting!)")
    else:
        print("   ✓ Degree 10 performs better (but may still overfit on new data)")
    
    print("\nE. PRACTICAL RECOMMENDATIONS:")
    print("   1. Start with Linear Regression as baseline")
    print("   2. If poor performance, try Polynomial Regression (degree 2 or 3)")
    print("   3. Use cross-validation to select optimal degree")
    print("   4. Regularize polynomial models to prevent overfitting")
    print("   5. Consider other nonlinear models if polynomial doesn't work well")
    
    print("\n" + "=" * 70)
    print("CONCLUSION")
    print("=" * 70)
    print(f"For this nonlinear dataset:")
    print(f"• Linear Regression explains {lr_metrics['r2']*100:.1f}% of variance")
    print(f"• Polynomial Regression (degree 2) explains {pr_metrics['r2']*100:.1f}% of variance")
    
    if pr_metrics['r2'] > lr_metrics['r2']:
        improvement = ((pr_metrics['r2'] - lr_metrics['r2']) / lr_metrics['r2']) * 100
        print(f"• Polynomial provides {improvement:.1f}% relative improvement")
        print("• RECOMMENDATION: Use Polynomial Regression for this data")
    else:
        print("• Linear Regression is sufficient for this data")
    
    print("\n" + "=" * 70)

# Run the comparison
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"Error: {e}")
        print("\nTroubleshooting:")
        print("1. Make sure you're using a Python 3.x environment")
        print("2. Online compilers usually support numpy")
        print("3. If numpy is missing, the code won't run")
        print("\nAlternative: Use Google Colab or Replit for guaranteed numpy support")
